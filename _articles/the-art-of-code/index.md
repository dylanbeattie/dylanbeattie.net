The first computer I ever used was an Amstrad CPC 6128. It was 1985, was seven years old, and I had no idea what I was doing. And the Amstrad 6128 was not a terribly powerful computer... it didn't have a joystick, it didn't have Space Invaders, it didn't even have a colour screen. But something about that machine fascinated me. It came with a couple of rather dull games, and a programming language called LOGO - and LOGO was amazing, because you could use it to make the computer draw things. I'd sit there for hours, figuring things out on scraps of paper, typing in commands from the manual - and trust me, the DR LOGO Amstrad Programmers Manual was not written for seven-year-olds - but when I cracked it, the thrill was addictive. I had made the machine do what I want.

That thrill has never gone away. The dopamine rush of solving the problem - the green test, the successful deployment, the moment where you finally figure out what's been causing that bug. And today, you're going to hear talks from some amazing and brilliant speakers about how to do all sorts of useful, important things with computers - microservices, information security, user experience design, caching. But first, I want to talk to you all about something I love very much. I want to talk to you all about art.

And, as Oscar Wilde famously said, "all art is quite useless." Well, maybe. I certainly wouldn't recommend you use any of the examples in this talk in your next production deployment. But I think art does have a purpose. It helps us think, it helps us feel, it helps us to understand. 

Douglas Adams once wrote that 'the function of art is to hold a mirror up to nature' – and art is all very well, but if we're going to hold a mirror up to nature, first we need to invent the mirror. And that's where science and technology come in. We humans have created machines to reveal the hidden world that's all around us - a world of microscopic monsters, plants, animals, entire ecosystems, too small to see with the naked eye; a world that was hidden right under our noses until the first microscopes gave us a new perspective on our reality. Who knew there was so much strangeness and beauty in the eye of a beetle, in salt crystals growing in a dish of soy sauce, in the spiral vessels inside the stem of a banana plant?

We've gazed back at our planet earth from the silent darkness of the moon, we've launched satellites to the timeless worlds of space, and telescopes that have showed us the furthest reaches of our galaxy and listened to the echo of the Big bang - the aftershocks of the very creation of our universe.

And within my lifetime, the power of the computer has given us a window into another secret world, hidden in the fabric of our reality, a world every bit as bizarre and beautiful as the others - a secret world of mathematics and information.

Throughout the 1970s, the journalist Martin Gardner had a regular column in Scientific American magazine, where each month he'd share mathematical puzzles and curiosities with his readers. Of all the columns Gardner ever published, the one that had the greatest impact was undoubtedly that published in the October 1970 edition, where he introduced the world to something called Conway's Game of Life. Created by the English mathematician John Conway, the Game of Life was one of the earliest examples of something we call a cellular automaton. 

As games go, Conway's Game of Life doesn't look like it has much to offer. It's a game with no players and only three rules, played on an infinite board. 

**[SLIDE: Conway's grid]**

The board is divided into square cells, which are either living or dead. If a living cell has fewer than two living neighbours, it dies of loneliness. If a cell has more than three living neighbours, it dies of overcrowding. And if a dead cell has exactly three living neighbours, it comes back to life. That's it. Those are the entire rules of the game. 

When Martin Gardner first published his column, readers would play around with the idea, drawing a couple of generations of Life on sheets of squared graph paper, looking for patterns. But the 1970s also coincided with the widespread availability of graphical computer systems. The first graphical computers were only available to those people lucky enough to be working at universities or research laboratories, but by the end of the decade we started to see the first true microcomputers - machines like the Apple II and the Commodore PET. 

And when those early computer hackers started using their computers to simulate Conway's Game of Life, they discovered a world of astonishing richness and beauty - all thanks to those three simple rules. In Gardner's original article, he published drawings of the five tetrominoes -four-cell configurations that will be instantly familar to anybody who's ever played Tetris.

On graph paper, they're dry, sterile, lifeless - like butterflies in a museum, displayed in a glass case. It wasn't until we ran Conway's game on the earliest graphical computers that we could appreciate the strange beauty that emerged from those three simple rules. One of the earliest patterns to be discovered was the glider - a stable configuration of six cells that repeats over and over, gliding across the infinite grid as it goes. The glider was just the first of a whole fleet of configurations known as spaceships, every bit as bizarre as anything from the archives of Star Trek or Babylon 5. 

One of the earliest questions posed about the Game of Life was - does it have limits? Do all patterns eventually stabilise, or are there configurations that will grow indefinitely? In 1970, a team led by the renowned computer scientist Bill Gosper discovered a configuration known as the glider gun - a stable pattern that repeats every 30 generations, launching a glider across the grid each time. And when combined with a pattern known as the eater - a stable configuration that consumes gliders - we get two states. Yes and no, on and off, true and false.

And it turns out that, given those binary states, we can create logic gates. Here's an AND gate modelled in Conway's Game of Life. Two glider guns A and B represent our inputs, with their state controlled by these two eaters. When A is false - no output. When B is false - no output. But when both A and B are true... the output is true. 

And once you've got logic gates, you can build circuits. And once you've got circuits, you can build computers. And once you've got computers, you can run programs. Including a rather fun program you might have heard about, called Conway's Game of Life.

Now, the Game of Life wasn't the only hidden universe that we could explore with our graphical computers. During the 1970s, researchers working in fields as diverse as weather forecasting, zoology and geometry had their first glimpes into an entirely new kind of science. Systems defined entirely defined by a handful of simple rules, that would go on to exhibit the most wonderfully complex and unpredictable behaviour. The study of these systems would soon become known as chaos theory. Today, chaos has all sorts of practical applications. We use it for long-range weather forecasting, for data compression, we use it to analyse stock market prices and study traffic congestion. But chaos would also lead us into another world of bizarre and beautiful mathematical structures - the set of shapes known as fractals.

To understand fractals and fractal geometry, you have to wrap your head around one of the weirdest things that exists in mathematics. Or maybe it doesn't exist - but that's kind of the point. You see, one upon a time a bunch of mathematicians were sat around talking about maths, and specifically square numbers and square roots. You multiply a number by itself - say, 2 x 2, or 5 x 5 - and the result is a square. And 2x2 is 4, so 4 is the square root of 2. But the weird thing is that -2 x -2 is ALSO 4. Not minus four. Plus four. If you're wondering why... well, it just is. There's probably another universe where it isn't, but in the universe we live in, that's how numbers work.

So if 2x2 is 4, and -2 x -2 is also four, what number can you multiply by itself to get minus four? And this is where it gets a bit weird. Because there is no such number. It does not exist. And this is the point where most scientists would go 'splendid, we've proved it doesn't exist. Good job, everyone.'

But not mathematicians. Oh no. Mathematicians go 'ok, it doesn't exist - no problem. Let's imagine it does' and they just sort of thunder on regardless... and what's weird is that this impossible, imaginary number, that they've just pulled out of thin air, actually works. The number they invented is called *i* – for imaginary – and *i* is defined to be the square root of -1. 2*i* is the square root of -4, 5*i* is the square root of -25, and so on. And as long as you're prepared to ignore the fact that all this is clearly impossible, you can use these imaginary numbers to do all sorts of fascinating mathematics.

There's a thing we call an Argand diagram, which is a sort of map of the real and imaginary numbers. The real numbers – you know, the ones you find in bank accounts and restaurant bills – go across the horizontal axis, like this. The imaginary numbers – which also come in positive and negative varieties – go along the vertical axis like this. And all this space here represents what are known as the complex numbers. Complex numbers are like project schedules - they have a real part, and an imaginary part, and they are absolutely no use in trying to predict what's going to happen next.

We can do arithmetic with complex numbers – we can add them, subtract them, divide and multiply them. It's kinda fiddly, because we have to break them into bits, do separate arithmetic on the bits, and then combine them together again - kinda like a very simple map/reduce algorithm. But we can do it.

Now, until fairly recently, mathematics was almost entirely concerned with solving equations. When you're doing your calculations with an abacus and a quill pen, it's not much fun to keep on doing the same thing over and over and not know whether it's going to lead anywhere. 

But in 1918, two French mathematicians - Gaston Julia and Pierre Fatou - published a paper entitled *Mémoire sur l'itération des fonctions rationnelles -* "on the iteration of rational functions". Now I have not read this paper - partly because it's well over a hundred pages of incredibly dense mathematics, but mainly because I can't read French - but I do know that one of the things Julia and Fatou discussed in it was what happens when you take a complex number, feed it into a simple function, take the result, and feed it back into the same function.

When we do this with real numbers, we get some pretty predictable behaviour. For example, let's take f(x) = x  +1, it's obvious what's going to happen - we're just going to count up 1, 2, 3, to infinity. And beyond, if you're a Buzz Lightyear fan. If we take f(x) = 2 - x, we get what's called a periodic function - start with 0, we get 2-0 = 2; put that back in, we get 2 - 2 = 0, and round and round we go.

 But when we do this with complex numbers, we get all sorts of complex, unpredictable behaviour. Let's take this simple example - some complex number z, and each time, we'll just map z onto z squared. If we start here = 0.7 + 0.8i - we sort of bounce around for a few iterations and then wheeeee! go flying off towards infinity.

But if we take a different starting point - let's move a tiny bit to the left here, 0.6 + 0.8i - we get something completely different. it bounces around the origin for a few iterations, then goes into this sort of weird, almost circular orbit...

What Julia and Fatou had discovered was remarkable, but in 1918 the only wayto explore this strange phenomenon was to do calculations longhand - cranking complex numbers over and over to see what happened. Analyzing a single result could take hours.

Their work was celebrated - Gaston Julia was awarded the Grand Prix de l'Academie des Sciences - but it would remain an obscure footnote in the world of mathematics right up until the 1970s, when Julia's ideas were rediscovered by this man - Benoit Mandelbrot. Mandelbrot was born in Warsaw in 1924; his family moved to France in 1936, and after World War 2 he emigrated to the United States.

Now Mandelbrot was an awesome guy, and he an awesome job. He was a research fellow at IBM, and whenever he got bored he'd pop off for a bit and teach at Harvard University.  Mandelbrot was fascinated by the mathematics of everyday life - “Clouds are not spheres, mountains are not cones, coastlines are not circles, and bark is not smooth, nor does lightning travel in a straight line.” Towards the end of the 1970s, Mandelbrot was studying something he called 'roughness', and in the course of his research he came across Gaston Julia's work on rational functions.

But Mandelbrot had something Julia didn't... thanks to his position at IBM, he had access to some of the most powerful computers in the world. Now, this is 1970s, where 'most powerful' meant kilobytes of RAM and perhaps a few thousand operations per second - but that was enough power to do in a few hours what would have taken Gaston Julia a lifetime. Instead of plotting a few painstaking data points at a time, Mandelbrot could pick one of Julia's functions and use the IBM computers to draw the entire data set.

But Mandelbrot had a better idea. What if he fed each point back to itself, over and over and over again? Choose a point on the complex plane - a pixel on our Argand diagram - square it, add the original value, square it again, add the original value again, and see what it does. If it vanishes off to infinity within a certain number of cycles, then we conclude that that point is NOT part of our set, so paint the pixel white… but if it doesn’t, paint it black, because that pixel is part of something that has been called the ‘most complex object in nature’ - and which most of us know as the Mandelbrot set.

The earliest visualizations of the Mandelbrot set were drawn using text-mode terminals, with blank spaces and @-signs standing in for black and white pixels, but as computers got faster and more powerful, we’ve been able to explore this strange and wonderful shape in astonishing detail. In the 1990s, computers became powerful enough that we could render the Mandelbrot set in full colour. 

Actually, that’s not true. Technically, the Mandelbrot SET is the bit in the middle - that sort of weird sideways snowman shape with all the crinkly bits - and what we’re doing is coloring the points immediately outside the set based on how quickly they diverge. Points that zoom off towards infinity within a few cycles are deep blue, points that take a little longer are purple, points that take longer still are red. 

 But the most astonishing thing about the Mandelbrot set is that the deeper we dig, and the harder we look, the more we discover. That simple formula = z => z^2 + z - is a doorway into infinite world of complexity and beauty. It never runs out. The video playing behind me is a deep zoom Mandelbrot sequence that starts with the entire set and magnifies it 10^20 times. At 10^1 times, the original set is the size of this room. At 100 times, it’s the size of the building. 100 times, the original set is the size of this city block. That’s a 100,000 times zoom. The original set is now larger than Nottingham. 10^7, the original set is bigger than the United Kingdom. 10^9, the set is bigger than Europe. 10^12, we’ve zoomed in so far that the original set we started with is now larger than planet Earth.

We'll stop here. We've zoomed in roughly 10 ^ 20 times. The original image is now so large it that to fly across it in a Boeing 747 would take a hundred billion years. 

Some people have referred to the Mandelbrot set as ’the fingerprint of God’. Now, I’m not a religious man, but I do find something deeply spiritual, and incredibly reassuring, about the Mandelbrot set. This astonishing shape, with its infinite depth and complexity and beauty, is not something anybody designed. It’s something we found. Something that has been hidden, buried inside the mathematics that shapes our universe. And it was buried WELL - to discover it, we had to invent arithmetic. We had to evolve imagination and creativity - the intellectual capacity to conjure imaginary numbers out of nowhere, to ignore the fact they’re impossible and learn how to manipulate them. And then we had to invent computers, thinking machines that could perform the thousands of millions of calculations a second that are required to map this amazing mathematical landscape and put it up on a screen where we can see it. Perhaps it is the fingerprint of God. Or maybe it's an Easter egg, hidden in the fabric of our reality to let us know we’re on the right track. Or maybe it's a glitch in the simulation and they're gonna fix it in the next patch release. But I hope not.

 Of course, this is 2019, and computer graphics is everywhere. There’s no longer anything remotely remarkable about seeing something on a computer screen - or a cinema screen, for that matter. In 1982, Disney released TRON, the first motion picture to use computer graphics to create special effects - and it seems incredible now, but the reason TRON didn’t win an Oscar is that it was banned from the visual effects category by the Academy, who considered that using computers to create movie special effects was ‘cheating’. In 1986, Pixar released Luxo, Jr - the first film created entirely using computer animation. From the dinosaurs in Jurassic Park, to Woody and Buzz in Disney’s Toy Story, to the epic fantasy battles of the Lord of the Rings movies, computer generated imagery - CGI - has smashed one milestone after another. In 2016, Industrial Light and Magic used CGI to create a digital version of the actor Peter Cushing, who died in 1994, and recent advances in machine learning and digital image manipulation have produced so-called ‘deep fake’ technology. It’s not inconceivable that within the next ten years you’ll be able to unlock your phone and say ‘hey Siri - make me a video of Friends except every character is played by Nicolas Cage’ and boom! it’ll just do it.   

But machine learning has also introduced us to another completely new kind of art, that’s that is every bit as bizarre and wonderful as fractals. 

 You ever lie on your back in a field as the clouds drifting by on a summer day, and watch as your mind turns the clouds into all sorts of recognisable shapes - a rabbit, a dog, a smiling face? That’s pattern recognition in action. It’s one of the things that our brains are really, really good at - and building machines that can do that same sort of pattern recognition is the fundamentel basis behind modern research into computer vision. We build things called convolutional neural networks - we build layers upon layers of algorithms, each looking for specific patterns, shapes and boundaries. Then we train them on thousands of photographs - these pictures are dogs, these pictures are not dogs - and the layers of algorithms reinforce and modify each other until we don’t really have any idea how they work any more. But if we’re lucky, we end up with an algorithm that can tell the difference between, say, a chihuaha and muffin. Which is actually a lot harder than it sounds.

 But what happens if we reverse the process? Let’s take our dog-detector and turn it into a dog amplifier - we’ll feed a random image into it, and get our neural network to find things that it thinks look like dogs, and then enhance the dogginess of those details… and then, in time-honoured tradition, we’ll take the output and feed it back into the same function. After a few iterations, things start to look a little weird. After a few dozen iterations, things get positively psychedelic.

The first program to do this was developed by Alexander Mordvintsev at Google in 2015. That program was known as Deep Dream; the technique it pioneered is now widely known as ‘deep dreaming’, and the bizarre, disturbing images it creates are known as ‘deep dreams’. The reason they’re so unsettling is that these kind of images fire all of those pattern recognition routines that are wired into our brains - they’re like walking through a forest at night, where every tree looks like a dangerous predator until you look at it properly and realise there’s actually nothing there. 

I have no doubt that as augmented and virtual reality become more widespread, we’re going to see more weird and wonderful computer-generated art; the power to create truly immersive environments opens the door to a whole new kind of experience - we’re no longer sitting looking at an image on a screen; the viewer becomes an integral part of the experience - and like almost every invention and innovation in history, there will be people who try to use it to make the world a better place, there will be people who try to to use it for gain and profit, and there will be people who just want to use it to create art, to push the boundaries of human experience just to see what it feels like.

Now, we’ve talked so far about using computers and computer programs to create art - you write the code, you run the code, the code produces some kind of output, and then you show people that output and everybody goes ‘wow, that’s amazing’. Or accuses you of cheating and bans you from the Oscars, if you’re the team who created TRON - I guess some people just don’t appreciate great art when they see it.

But what about if running the code isn’t the point? What if the code itself is an art form? Our industry has debated this for years… from the title of Donald Knuth’s classic book ’The Art of Computer Programming’, first published in 1968, through ongoing discussions around whether programming is a scientific, or a creative, or an engineering discipline… but even those kinds of discussion tend to be about writing useful programs that do things. And I don’t know about you, but sometimes I get a bit bored of talking about refactoring microservices and HTTP caching headers… and that’s when I go looking for crazy code. Code that not only doesn’t do anything important, but code that’s so wonderfully weird that if anybody actually tried to use it in production, they’d probably get fired. 

I’m sure you’ve all had those days when you’re digging through some legacy code or reviewing a pull request and thinking ‘what the heck does this even DO?’ - and then eventually it clicks, and you’re like, ‘but… why? Why would anybody EVER write code like this?’ Well, imagine if there were people out there who were actually writing that kind of code, on purpose… to win a contest.

The first International Obfuscated C Code contest was held in 1984. The rules of the contest were simple: your source code file had to be a C program no more than 512 bytes long, with credit awarded for “violations of structured programming and non-clarity”. 

The contest has been held almost every year since then - and some of the entries are absolutely delightful. Here’s a wonderful example from 2015. Here’s the source code. The ENTIRE source code. That’s it. Do you know what it does? No, of course you don’t… to work out what’s going on you’d have to spend at least a couple of hours trying to figure out what it actually does. So let’s build it and run it… and, of course! It’s Flappy Bird. In a Unix terminal window.

Or how about this one, submitted by Stijn Walters back in 2001? This one's actually a pretty straightfoward Mandelbrot set generator written in C... oh, except that the source code itself is shaped like a Mandelbrot set.

 I love obfuscated coding contests, not just because they’re fun, but because every year or two, as computers and networks and standards get more sophisticated, people come up with new and wonderful ways of doing incredibly clever things with the most minimal amounts of code. This next example is in JavaScript - that’s it. That’s the entire application - there’s no jQuery, no React or Angular or external libraries, no nested DIVs. And here’s what you get when you open that in a browser. It’s called Nanochess, by a guy called Oscar Toledo G. Oscar has won the Obfuscated C contest more times than I can count - he won in 2005 with a C version of his chess program, written in less than 1k of C code - you wanna see the source code? Isn’t that special? And now, fourteen years later, thanks to things like having the entire set of chess symbols built in to Unicode, it’s possible to create a chess game in less than a kilobyte of JavaScript. And when I say a chess game, I mean a game that can actually beat me at chess. In a kilobyte of JavaScript.

Now, in 1994, Szymon Rusinkiewicz won an honorable mention in the International Obfuscated C contest for submitting a singular and remarkable program. The program was called smr.c, and it claimed to be the world’s smallest-ever self-reproducing program. Here’s the source code for smr.c: 

Yep. It’s an empty file. And it worked because, back in 1994, there were a handful of C compilers floating around that, if you asked them to compile an empty file, they’d produce a valid program which created no output - and so, in a manner of speaking, Rusinkiewicz had indeed created a program that produced its own output. 

And that’s actually a fascinating idea. Take a moment to think about it - in your favourite programming language, could you write a program that produced its own source code as output? Now, you can’t cheat - you can’t just read the source code file and write that back to STDOUT. That wouldn’t be any fun at all… 

A program that produces its own source code as output is known as a quine - the name was coined by Douglas Hofstadter in his book ‘Godel, Escher and Bach’ in 1979, after a guy called Willard Van Orman Quine, an American philosopher and mathematician, who studied self-referential logical statements. 

Quines are, of course, completely useless - but creating them can be a lot harder than it looks. Let’s build a quine in C#. 

OK, so it’s a Program, with a static void main() method, that’s going to print… what? 

Well, it needs to print a Program, obviously. With a static void main() method, that prints a program, that prints a static void main method, that… 

But hang on a second. What if we put our program into a string, and print it twice? Well, it turns out in .NET we can use a feature called string templating to print a string that contains itself - effectively saying “hey, print this string S, but when you see {0}, replace it with the first argument” - which is, of course, the string S. 

Now, some languages are better suited to writing quines than others. JavaScript, for example, is brilliant for writing quines, because JavaScript functions can refer to themselves, AND have a built-in toString() method that’ll return their own source code as a rather convenient string. If you’re running ES6 JavaScript, you can actually create a quine in 21 bytes: 

But one of my favourite examples ever was created by a guy called Leon Bambrick, who set out to answer the question - can you create a quine using HTML and CSS? That is, can you create a web page that renders its own source code? 

Well, let’s have a go. 

